
@article{large_dynamics_1999,
	title = {The dynamics of attending: {How} people track time-varying events},
	volume = {106},
	issn = {1939-1471},
	shorttitle = {The dynamics of attending},
	doi = {10.1037/0033-295X.106.1.119},
	abstract = {A theory of attentional dynamics is proposed and aimed at explaining how listeners respond to systematic change in everyday events while retaining a general sense of their rhythmic structure. The approach describes attending as the behavior of internal oscillations, called attending rhythms, that are capable of entraining to external events and targeting attentional energy to expected points in time. A mathematical formulation of the theory describes internal oscillations that focus pulses of attending energy and interact in various ways to enable attentional tracking of events with complex rhythms. This approach provides reliable predictions about the role of attending to event time structure in rhythmical events that modulate in rate, as demonstrated in 3 listening experiments. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Psychological Review},
	author = {Large, Edward W. and Jones, Mari Riess},
	year = {1999},
	keywords = {Attention, Human Biological Rhythms, Mathematical Modeling, Pattern Discrimination, Time Perception, Theories},
	pages = {119--159},
}

@article{large_dynamic_2023,
	title = {Dynamic models for musical rhythm perception and coordination},
	volume = {17},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2023.1151895},
	doi = {10.3389/fncom.2023.1151895},
	abstract = {Rhythmicity permeates large parts of human experience. Humans generate various motor and brain rhythms spanning a range of frequencies. We also experience and synchronize to externally imposed rhythmicity, for example from music and song or from the 24-hour light-dark cycles of the sun. In the context of music, humans have the ability to perceive, generate, and anticipate rhythmic structures, for example, “the beat”. Experimental and behavioral studies offer clues about the biophysical and neural mechanisms that underlie our rhythmic abilities, and about different brain areas that are involved but many open questions remain. In this paper, we review several theoretical and computational approaches, each centered at different levels of description, that address specific aspects of musical rhythmic generation: perception, attention, perception-action coordination, and learning. We survey methods and results from applications of dynamical systems theory, neuro-mechanistic modeling, and Bayesian inference. Some frameworks rely on synchronization of intrinsic brain rhythms that span the relevant frequency range; some formulations involve real-time adaptation schemes for error-correction to align the phase and frequency of a dedicated circuit; others involve learning and dynamically adjusting expectations to make rhythm tracking predictions. Each of the approaches, while initially designed to answer specific questions, offers the possibility of being integrated into a larger framework that provides insights into our ability to perceive and generate rhythmic patterns.},
	language = {English},
	urldate = {2024-06-18},
	journal = {Frontiers in Computational Neuroscience},
	author = {Large, Edward W. and Roman, Iran and Kim, Ji Chul and Cannon, Jonathan and Pazdera, Jesse K. and Trainor, Laurel J. and Rinzel, John and Bose, Amitabha},
	month = may,
	year = {2023},
	keywords = {beat perception, entrainment, neuro-mechanistic modeling, dynamical systems, Music, Bayesian modeling, synchronization},
}

@inproceedings{nakamura_performance_2017,
	title = {Performance {Error} {Detection} and {Post}-{Processing} for {Fast} and {Accurate} {Symbolic} {Music} {Alignment}},
	url = {https://www.semanticscholar.org/paper/Performance-Error-Detection-and-Post-Processing-for-Nakamura-Yoshii/37e9f5e23cada918c2b8982d71a18972140d9d5a},
	abstract = {This paper presents a fast and accurate alignment method for polyphonic symbolic music signals. It is known that to accurately align piano performances, methods using the voice structure are needed. However, such methods typically have high computational cost and they are applicable only when prior voice information is given. It is pointed out that alignment errors are typically accompanied by performance errors in the aligned signal. This suggests the possibility of correcting (or realigning) preliminary results by a fast (but not-so-accurate) alignment method with a refined method applied to limited segments of aligned signals, to save the computational cost. To realise this, we develop a method for detecting performance errors and a realignment method that works fast and accurately in local regions around performance errors. To remove the dependence on prior voice information, voice separation is performed to the reference signal in the local regions. By applying our method to results obtained by previously proposed hidden Markov models, the highest accuracies are achieved with short computation time. Our source code is published in the accompanying web page, together with a user interface to examine and correct alignment results.},
	urldate = {2024-06-18},
	author = {Nakamura, Eita and Yoshii, Kazuyoshi and Katayose, H.},
	year = {2017},
}

@misc{hu_batik-plays-mozart_2023,
	title = {The {Batik}-plays-{Mozart} {Corpus}: {Linking} {Performance} to {Score} to {Musicological} {Annotations}},
	shorttitle = {The {Batik}-plays-{Mozart} {Corpus}},
	url = {http://arxiv.org/abs/2309.02399},
	doi = {10.48550/arXiv.2309.02399},
	abstract = {We present the Batik-plays-Mozart Corpus, a piano performance dataset combining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored B{\textbackslash}"osendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can further be connected to the musicological annotations (harmony, cadences, phrases) on these scores that were recently published by Hentschel et al. (2021). The result is a high-quality, high-precision corpus mapping scores and musical structure annotations to precise note-level professional performance information. As the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects. In the paper, we outline the curation process of the alignment and conduct two exploratory experiments to demonstrate its usefulness in analyzing expressive performance.},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Hu, Patricia and Widmer, Gerhard},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02399 [cs, eess]},
	keywords = {Computer Science - Sound, Computer Science - Digital Libraries, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{kosta_mazurkabl:_2018,
	title = {{MazurkaBL}: {Score}-aligned {Loudness}, {Beat}, and {Expressive} {Markings} {Data} for 2000 {Chopin} {Mazurka} {Recordings}},
	shorttitle = {{MazurkaBL}},
	url = {https://zenodo.org/records/1290763},
	abstract = {Large-scale analysis of expressive performance—with focus on how a performer responds to score markings---has been limited by a lack of big datasets of recordings with accurate beat and loudness information with score markings. To bridge this gap, we created the MazurkaBL dataset, a collection of score-beat positions and loudness values, with corresponding score dynamic and tempo markings for 2000 recordings of forty-four Chopin Mazurkas. MazurkaBL forms the largest annotated expressive performance dataset to date. This paper describes how the dataset was created, and variations found in the dataset. For each Mazurka, the recordings were first aligned to the score and one to another to facilitate the transfer of meticulously created manual beat annotations from one reference to all other recordings. We propose a multi-recording alignment heuristic that optimises the reference audio choice for best average alignment results. Loudness values in sones are extracted and analysed; we also provide the score position of dynamic and tempo markings. The result is a rich repository of score-aligned loudness, beat, and expressive marking data for studying expressive variations. We further discuss recent and future applications of MazurkaBL and future directions for database development.},
	urldate = {2024-06-18},
	collaborator = {Kosta, Katerina and Bandtlow, Oscar F. and Chew, Elaine},
	month = may,
	year = {2018},
	doi = {10.5281/zenodo.1290763},
}

@article{hentschel_annotated_2021,
	title = {The {Annotated} {Mozart} {Sonatas}: {Score}, {Harmony}, and {Cadence}},
	volume = {4},
	shorttitle = {The {Annotated} {Mozart} {Sonatas}},
	url = {https://transactions.ismir.net/articles/10.5334/tismir.63},
	doi = {10.5334/tismir.63},
	abstract = {This article describes a new expert-labelled dataset featuring harmonic, phrase, and cadence analyses of all piano sonatas by W.A. Mozart. The dataset draws on the DCML standard for harmonic annotation and is being published adopting the FAIR principles of Open Science. The annotations have been verified using a data triangulation procedure which is presented as an alternative approach to handling annotator subjectivity. This procedure is suited for ensuring consistency, within the dataset and beyond, despite the high level of analytical detail afforded by the employed harmonic annotation syntax. The harmony labels also encode contextual information and are therefore suited for investigating music theoretical questions related to tonal harmony and the harmonic makeup of cadences in the classical style. Apart from providing basic statistical analyses characterizing the dataset, its music theoretical potential is illustrated by two preliminary experiments, one on the terminal harmonies of cadences and the other on the relation between performance durations and harmonic density. Furthermore, particular features can be selected to produce more coarse-grained training data, for example for chord detection algorithms that require less analytical detail. Facilitating the dataset’s reusability, it comes with a Python script that allows researchers to easily access various representations of the data tailored to their particular needs.},
	language = {en-US},
	number = {1},
	urldate = {2024-06-18},
	author = {Hentschel, Johannes and Neuwirth, Markus and Rohrmeier, Martin},
	month = may,
	year = {2021},
	pages = {67--80},
}

@article{muller_memory-restricted_nodate,
	title = {{MEMORY}-{RESTRICTED} {MULTISCALE} {DYNAMIC} {TIME} {WARPING}},
	url = {https://www.academia.edu/25724042/MEMORY_RESTRICTED_MULTISCALE_DYNAMIC_TIME_WARPING},
	abstract = {Dynamic Time Warping (DTW) is an established method for finding a global alignment between two feature sequences. However, having a computational complexity that is quadratic in the input length, memory consumption becomes a major issue when dealing},
	urldate = {2024-06-18},
	author = {Müller, Meinard},
}

@inproceedings{foscarin_asap:_2020,
	title = {{ASAP}: a dataset of aligned scores and performances for piano transcription},
	shorttitle = {{ASAP}},
	url = {https://cnam.hal.science/hal-02929324},
	abstract = {In this paper we present Aligned Scores and Performances (ASAP): a new dataset of 222 digital musical scores aligned with 1068 performances (more than 92 hours) of Western classical piano music. The scores are provided as paired MusicXML files and quantized MIDI files, and the performances as paired MIDI files and partially as audio recordings. Scores and performances are aligned with downbeat, beat, time signature, and key signature annotations. ASAP has been obtained thanks to a new annotation workflow that combines score analysis and alignment algorithms, with the goal of reducing the time for manual annotation. The dataset itself is, to our knowledge, the largest that includes an alignment of music scores to MIDI and audio performance data. As such, it is a useful resource for a wide variety of MIR applications, from those that target the complete audio-to-score Automatic Music Transcription task, to others that target more specific aspects (e.g., key signature estimation and beat or downbeat tracking from both MIDI and audio representations).},
	language = {en},
	urldate = {2024-06-18},
	author = {Foscarin, Francesco and Mcleod, Andrew and Rigaux, Philippe and Jacquemard, Florent and Sakai, Masahiko},
	month = oct,
	year = {2020},
}

@article{peter_automatic_2023,
	title = {Automatic {Note}-{Level} {Score}-to-{Performance} {Alignments} in the {ASAP} {Dataset}},
	volume = {6},
	url = {https://transactions.ismir.net/articles/10.5334/tismir.149},
	doi = {10.5334/tismir.149},
	abstract = {Several MIR applications require fine-grained note alignments between MIDI performances and their musical scores for training and evaluation. However, large and high-quality datasets with this kind of data are not available, and their manual creation is a very time-consuming task that can only be performed by field experts. In this paper, we evaluate state-of-the-art automatic note alignment models applied to dataset generation. We increase the accuracy and reliability of the produced alignments with models that flexibly leverage existing annotations such as beat or measure alignments. We thoroughly evaluate these segment-constrained models and use the best to create note alignments for the ASAP dataset, a large dataset of solo piano MIDI performances beat-aligned to MusicXML scores. The resulting note alignments are manually checked and publicly available at: https://github.com/CPJKU/asap-dataset. The contributions of this paper are four-fold: (1) we extend the ASAP dataset with reliable note alignments, thus creating (n)ASAP, the largest available fully note-aligned dataset, comprising more than 7 M annotated notes and close to 100 hours of music; (2) we design, evaluate, and publish segment-constrained models for note alignments that flexibly leverage existing annotations and significantly outperform automatic models; (3) we design, evaluate, and publish unconstrained automatic models for note alignment that produce results on par with the state of the art; (4) we introduce Parangonada, a web-interface for visualizing and correcting alignment annotations.},
	language = {en-US},
	number = {1},
	urldate = {2024-06-18},
	author = {Peter, Silvan David and Cancino-Chacón, Carlos Eduardo and Foscarin, Francesco and McLeod, Andrew Philip and Henkel, Florian and Karystinaios, Emmanouil and Widmer, Gerhard},
	month = jun,
	year = {2023},
	pages = {27--42},
}

@article{loehr_temporal_2011,
	title = {Temporal coordination and adaptation to rate change in music performance},
	volume = {37},
	issn = {1939-1277},
	doi = {10.1037/a0023102},
	abstract = {People often coordinate their actions with sequences that exhibit temporal variability and unfold at multiple periodicities. We compared oscillator- and timekeeper-based accounts of temporal coordination by examining musicians' coordination of rhythmic musical sequences with a metronome that gradually changed rate at the end of a musical phrase (Experiment 1) or at the beginning of a phrase (Experiment 2). The rhythms contained events that occurred at the same periodic rate as the metronome and at half the period. Rate change consisted of a linear increase or decrease in intervals between metronome onsets. Musicians coordinated their performances better with a metronome that decreased than increased in tempo (as predicted by an oscillator model), at both beginnings and ends of musical phrases. Model performance was tested with an oscillator period or timekeeper interval set to the same period as the metronome (1:1 coordination) or half the metronome period (2:1 coordination). Only the oscillator model was able to predict musicians' coordination at both periods. These findings suggest that coordination is based on internal neural oscillations that entrain to external sequences.},
	language = {eng},
	number = {4},
	journal = {Journal of Experimental Psychology. Human Perception and Performance},
	author = {Loehr, Janeen D. and Large, Edward W. and Palmer, Caroline},
	month = aug,
	year = {2011},
	pmid = {21553990},
	keywords = {Adaptation, Psychological, Adolescent, Adult, Algorithms, Analysis of Variance, Female, Humans, Imitative Behavior, Male, Models, Neurological, Music, Pattern Recognition, Physiological, Periodicity, Reaction Time, Reference Values, Time Perception, Young Adult},
	pages = {1292--1309},
}

@article{schulze_keeping_2005,
	title = {Keeping {Synchrony} {While} {Tempo} {Changes}: {Accelerando} and {Ritardando}},
	volume = {22},
	issn = {0730-7829},
	shorttitle = {Keeping {Synchrony} {While} {Tempo} {Changes}},
	url = {https://www.jstor.org/stable/10.1525/mp.2005.22.3.461},
	doi = {10.1525/mp.2005.22.3.461},
	abstract = {We studied synchronization with a metronome that smoothly changes tempo, from slow to fast (accelerando) or from fast to slow (ritardando). During the transition phase, systematic alternations of underadjustment and overadjustment of period and phase were observed. We analyzed the synchronization error (�asynchrony�) sequences in terms of two models that both assume linear period and phase correction mechanisms but differ in terms of how the timekeeper period is adjusted to the tempo change. In the interval-based model, period corrections are based on comparisons between timekeeper and metronome intervals, whereas in the asynchrony-based model, period corrections are based on the deviations of taps from metronome events. The qualitative data pattern is more compatible with an asynchrony-based model than with an interval-based model. Additional mechanisms that switch on and off period adjustment seem to be needed, however, for a quantitative fit of this model to our data.},
	number = {3},
	urldate = {2024-06-18},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Schulze, Hans-Henning and Cordes, Andreas and Vorberg, Dirk},
	year = {2005},
	pages = {461--477},
}

@article{murphy_quantization_2011,
	title = {Quantization revisited: a mathematical and computational model},
	volume = {5},
	issn = {1745-9737, 1745-9745},
	shorttitle = {Quantization revisited},
	url = {http://www.tandfonline.com/doi/abs/10.1080/17459737.2011.573674},
	doi = {10.1080/17459737.2011.573674},
	language = {en},
	number = {1},
	urldate = {2024-06-19},
	journal = {Journal of Mathematics and Music},
	author = {Murphy, Declan},
	month = mar,
	year = {2011},
	pages = {21--34},
}

@inproceedings{romero-garcia_model_2022,
	title = {A {Model} of {Rhythm} {Transcription} as {Path} {Selection} through {Approximate} {Common} {Divisor} {Graphs}},
	url = {https://hal.science/hal-03714207},
	abstract = {We apply the concept of approximated common divisors (ACDs) to estimate the tempo and quantize the durations of a rhythmic sequence. The ACD models the duration of the tatum within the sequence, giving its rate in beats per minute. The rhythm input, a series of timestamps, is first split into overlapping frames. Then, we compute the possible ACDs that fit this frame and build a graph with the candidate ACDs as nodes. By building this graph, we transform the quantization problem into one of path selection, where the nodes represent the ACDs and determine the note values of the transcription and the edges represent tempo transitions between frames. A path through the graph thus corresponds to a rhythm transcription. For path selection, we present both an automated method using weights for evaluating the transcription and finding the shortest path, and an interactive approach that gives users the possibility of influencing the path selection.},
	language = {en},
	urldate = {2024-06-19},
	author = {Romero-García, Gonzalo and Guichaoua, Corentin and Chew, Elaine},
	month = may,
	year = {2022},
}

@article{nakamura_outer-product_2014,
	title = {Outer-{Product} {Hidden} {Markov} {Model} and {Polyphonic} {MIDI} {Score} {Following}},
	volume = {43},
	issn = {0929-8215, 1744-5027},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09298215.2014.884145},
	doi = {10.1080/09298215.2014.884145},
	language = {en},
	number = {2},
	urldate = {2024-06-19},
	journal = {Journal of New Music Research},
	author = {Nakamura, Eita and Nakamura, Tomohiko and Saito, Yasuyuki and Ono, Nobutaka and Sagayama, Shigeki},
	month = apr,
	year = {2014},
	pages = {183--201},
}

@article{shibata_non-local_2021,
	title = {Non-local musical statistics as guides for audio-to-score piano transcription},
	volume = {566},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025521002516},
	doi = {10.1016/j.ins.2021.03.014},
	language = {en},
	urldate = {2024-06-20},
	journal = {Information Sciences},
	author = {Shibata, Kentaro and Nakamura, Eita and Yoshii, Kazuyoshi},
	month = aug,
	year = {2021},
	pages = {262--280},
}

@article{nakamura_stochastic_2015,
	title = {A {Stochastic} {Temporal} {Model} of {Polyphonic} {MIDI} {Performance} with {Ornaments}},
	volume = {44},
	issn = {0929-8215, 1744-5027},
	url = {http://www.tandfonline.com/doi/full/10.1080/09298215.2015.1078819},
	doi = {10.1080/09298215.2015.1078819},
	language = {en},
	number = {4},
	urldate = {2024-06-20},
	journal = {Journal of New Music Research},
	author = {Nakamura, Eita and Ono, Nobutaka and Sagayama, Shigeki and Watanabe, Kenji},
	month = oct,
	year = {2015},
	pages = {287--304},
}

@article{raphael_probabilistic_2001,
	title = {A {Probabilistic} {Expert} {System} for {Automatic} {Musical} {Accompaniment}},
	volume = {10},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/106186001317115081},
	doi = {10.1198/106186001317115081},
	language = {en},
	number = {3},
	urldate = {2024-06-20},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Raphael, Christopher},
	month = sep,
	year = {2001},
	pages = {487--512},
}

@article{antescofo,
  TITLE = {{Antescofo {\`a} l'avant-garde de l'informatique musicale}},
  AUTHOR = {Cont, Arshia and Jacquemard, Florent and Gaumin, Pierre-Olivier},
  URL = {https://inria.hal.science/hal-00753014},
  JOURNAL = {{Interstices}},
  PUBLISHER = {{INRIA}},
  YEAR = {2012},
  MONTH = Nov,
  HAL_ID = {hal-00753014},
  HAL_VERSION = {v1},
}

@inproceedings{peter2023sounding,
  title={Sounding Out Reconstruction Error-Based Evaluation of Generative Models of Expressive Performance},
  author={Peter, Silvan David and Cancino-Chac{\'o}n, Carlos Eduardo and Karystinaios, Emmanouil and Widmer, Gerhard},
  booktitle={Proceedings of the 10th International Conference on Digital Libraries for Musicology},
  pages={58--66},
  year={2023}
}

@article{Kosta2016Mapping,
author = {Katerina Kosta, Rafael Ramírez, Oscar F. Bandtlow and Elaine Chew},
title = {Mapping between dynamic markings and performed loudness: a machine learning approach},
journal = {Journal of Mathematics and Music},
volume = {10},
number = {2},
pages = {149--172},
year = {2016},
publisher = {Taylor \& Francis},
doi = {10.1080/17459737.2016.1193237},
url = {https://doi.org/10.1080/17459737.2016.1193237},
eprint = {https://doi.org/10.1080/17459737.2016.1193237}
}

@article{schreiber_music_2020,
	title = {Music {Tempo} {Estimation}: {Are} {We} {Done} {Yet}?},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	issn = {2514-3298},
	shorttitle = {Music {Tempo} {Estimation}},
	url = {https://transactions.ismir.net/article/10.5334/tismir.43/},
	doi = {10.5334/tismir.43},
	number = {1},
	urldate = {2024-07-05},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Schreiber, Hendrik and Urbano, Julián and Müller, Meinard},
	month = aug,
	year = {2020},
	pages = {111},
}
